{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\jimbl\\\\Git\\\\LELPerformance\\\\Network_Measurement_Project\\\\Jupyter', 'C:\\\\Users\\\\jimbl\\\\Git\\\\PyUtils\\\\lib', 'C:\\\\Users\\\\jimbl\\\\.conda\\\\envs\\\\py38\\\\python38.zip', 'C:\\\\Users\\\\jimbl\\\\.conda\\\\envs\\\\py38\\\\DLLs', 'C:\\\\Users\\\\jimbl\\\\.conda\\\\envs\\\\py38\\\\lib', 'C:\\\\Users\\\\jimbl\\\\.conda\\\\envs\\\\py38', '', 'C:\\\\Users\\\\jimbl\\\\.conda\\\\envs\\\\py38\\\\lib\\\\site-packages', 'C:\\\\Users\\\\jimbl\\\\.conda\\\\envs\\\\py38\\\\lib\\\\site-packages\\\\GDAL-3.0.2-py3.8-win-amd64.egg', 'C:\\\\Users\\\\jimbl\\\\.conda\\\\envs\\\\py38\\\\lib\\\\site-packages\\\\win32', 'C:\\\\Users\\\\jimbl\\\\.conda\\\\envs\\\\py38\\\\lib\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\jimbl\\\\.conda\\\\envs\\\\py38\\\\lib\\\\site-packages\\\\Pythonwin']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "print(sys.path)\n",
    "import subprocess\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import calendar\n",
    "import traceback\n",
    "import json\n",
    "from pyutils import *\n",
    "from pdutils import *\n",
    "from pdpltutils import *\n",
    "from gputils import *\n",
    "from iputils import *\n",
    "import xmltodict\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jimbl\\Downloads\\ exists\n",
      "C:\\Users\\jimbl\\Box\\BoxDesktop\\Documents\\LEL\\Network_Measurements\\Network_Study\\All_Measures exists\n",
      "C:\\Users\\jimbl\\Box\\BoxDesktop\\Documents\\LEL\\Network_Measurements\\Network_Study\\cbrs5G_2024-04-19-20 exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jimbl\\.conda\\envs\\py38\\lib\\site-packages\\pandas\\core\\dtypes\\cast.py:130: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n",
      "  arr = construct_1d_object_array_from_listlike(values)\n",
      "C:\\Users\\jimbl\\.conda\\envs\\py38\\lib\\site-packages\\pyproj\\crs\\crs.py:53: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  return _prepare_from_string(\" \".join(pjargs))\n"
     ]
    }
   ],
   "source": [
    "if sys.platform == \"linux\":\n",
    "    IP2DIR=\"/home/jblake1/Downloads/Network_Measurements\"\n",
    "    DATADIR=\"/home/jblake1/Downloads/Network_Measurements\"\n",
    "    DATADIR2=\"/home/jblake1/LEL_Network_Project\"\n",
    "else:\n",
    "    IP2DIR=\"C:\\\\Users\\\\jimbl\\\\Downloads\\\\\"\n",
    "    DATADIR=\"C:\\\\Users\\\\jimbl\\\\Downloads\\\\\"\n",
    "    # DATADIR2=\"C:\\\\Users\\\\jimbl\\\\Box\\\\BoxDesktop\\\\Documents\\\\LEL\\\\Network_Measurements\\\\Network_Study\\\\ping-iperf-traceroute-2024-03-22-24\"\n",
    "    # DATADIR2=\"C:\\\\Users\\\\jimbl\\\\Box\\\\BoxDesktop\\\\Documents\\\\LEL\\\\Network_Measurements\\\\Network_Study\\\\LEL_Measures_03-22-2024_to_04_02_2024\"\n",
    "    # DATADIR2=\"C:\\\\Users\\\\jimbl\\\\Box\\\\BoxDesktop\\\\Documents\\\\LEL\\\\Network_Measurements\\\\Network_Study\\\\CBRS-5G-Ping-First-Look\"\n",
    "    DATADIR2=\"C:\\\\Users\\\\jimbl\\\\Box\\\\BoxDesktop\\\\Documents\\\\LEL\\\\Network_Measurements\\\\Network_Study\\\\cbrs5G_2024-04-19-20\"\n",
    "    ALLMEASDIR=\"C:\\\\Users\\\\jimbl\\\\Box\\\\BoxDesktop\\\\Documents\\\\LEL\\\\Network_Measurements\\\\Network_Study\\\\All_Measures\"\n",
    "print(f\"{IP2DIR} exists\") if os.path.isdir(IP2DIR) else print(f\"{IP2DIR} does not exist\")\n",
    "print(f\"{ALLMEASDIR} exists\") if os.path.isdir(ALLMEASDIR) else print(f\"{ALLMEASDIR} does not exist\")\n",
    "print(f\"{DATADIR2} exists\") if os.path.isdir(DATADIR2) else print(f\"{DATADIR2} does not exist\")\n",
    "IP2LITE=\"IP2LOCATION-LITE-DB11.CSV\"\n",
    "\n",
    "\n",
    "''' US Boundaries '''\n",
    "northern=49.382808 # latitude\n",
    "southern=24.521208\n",
    "eastern=-66.945392 # longitude\n",
    "western=-124.736342\n",
    "pointdict = {\n",
    " 'northeast':[northern,eastern],\n",
    " 'northwest':[northern,western],\n",
    " 'southeast':[southern,eastern],\n",
    " 'southwest':[southern,western]\n",
    "}\n",
    "tdfb = pd.DataFrame.from_dict(pointdict,orient='index',columns=['latitude','longitude'])\n",
    "tdfb['geometry'] = pt2geom(tdfb,latcol='latitude',lngcol='longitude')\n",
    "usagb = df2gp(tdfb.copy())\n",
    "# To plot this map first:\n",
    "# ax= gp_plotPoints(usagb,mapon=True, alpha=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def histplot(dfin,title='Unknown Title',ax=None,filename='tmp.png', \n",
    "    figsize=(10,10), xlabel='',ylabel='',tabon=True, saveon=False,\n",
    "    bins=10, alpha=0.5, fontsize = 30, yticks = True,\n",
    "    tabfontsize = 30, tabsizex = 1,tabsizey=2,**kwargs):\n",
    "    font = {'size':fontsize}\n",
    "    matplotlib.rc('font',**font)\n",
    "    df = pd.DataFrame(dfin) # in case actually a series\n",
    "    ''' Parameters '''\n",
    "    if ax is None:\n",
    "        ax = plt.figure(figsize=figsize).add_subplot(111)\n",
    "    ''' Plot '''\n",
    "    ax = df.plot.hist(bins=bins,alpha=alpha,title=title,figsize=figsize,ax=ax,**kwargs)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    if not yticks: ax.set_yticklabels([])\n",
    "    # print(tabon)\n",
    "    if tabon:\n",
    "        tabcolWidths = [0.2]\n",
    "        tab = table(ax,np.round(df.describe(),2),loc='upper right',colWidths=tabcolWidths)\n",
    "        tab.set_fontsize(tabfontsize)\n",
    "        tab.scale(tabsizex,tabsizey)\n",
    "    if saveon:\n",
    "        print(\"Saving %s\" % filename)\n",
    "        savePlot(ax,filename)\n",
    "    return ax\n",
    "def makeHist(fdf,title=\"UNKNOWN\", filename=\"tmp.png\"):\n",
    "    collst = fdf.columns.sort_values()\n",
    "    col0 = collst[0]\n",
    "    coln = collst[len(collst)-1]\n",
    "    # print(col0)\n",
    "    ax = histplot(fdf[col0],tabon=False, legend=False)\n",
    "    for col in collst[1:-1]:\n",
    "        ax = histplot(fdf[col],ax=ax,tabon=False, legend=False, label=\"PING\",by=None)\n",
    "    ax = histplot(fdf[coln],ax=ax,tabon=False, title=title,saveon=False)\n",
    "    ax.legend(fontsize=\"20\",loc=\"right\")\n",
    "    savePlot(ax,filename)\n",
    "# tabcolWidths = [0.1]\n",
    "# tab = table(ax,np.round(tdfy.describe(),2),loc='bottom')\n",
    "# # tab.set_fontsize(30)\n",
    "# # tab.scale(10,10)\n",
    "\n",
    "def plotMap(fgp,title=\"NONE\",saveon=False,filename=\"tmp.png\"):\n",
    "    tablecolumns = ['city_name','region_name','IP']\n",
    "    tabdf = fgp[tablecolumns]\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "    # plt.figure(figsize=(20,20))\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.subplots_adjust(left=0.3, right=0.5, bottom=0.3, top=0.5)\n",
    "    # fig.subplots_adjust(hspace=30.0)\n",
    "    ax= gp_plotPoints(usagb,ax=ax, mapon=True, alpha=0)\n",
    "    ax = gp_plotPoints(fgp,ax=ax, mapon=True,c='red',title=title)\n",
    "    fgp = fgp[:-1] # Remove the last line -- belongs to next group\n",
    "    ax = gp_plotLines(fgp,mapon=True,geocol='LINEGEO',ax=ax,color='green')\n",
    "    table = ax.table(cellText=tabdf.values, colLabels=tabdf.columns,cellLoc='center', loc='bottom')\n",
    "    table.scale(1,1)\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    if saveon:\n",
    "        savePlot(ax,filename)\n",
    "\n",
    "def gp_plotLines(fgp,ax = None,geocol='geometry',ptype='Lines',mapon=False, title=None,\n",
    "                  figsize=(10,10),ctxprovider=ctxprovider,**kwargs):\n",
    "    ''' GeoPandas DataFrame with 'geometry' containing LineStrings '''\n",
    "    ''' Note: color parameter is 'color' '''\n",
    "    if geocol != 'geometry':\n",
    "        fgp['geometry'] = fgp[geocol]    \n",
    "    if mapon:\n",
    "        fgp = fgp.to_crs(epsg=3857)\n",
    "    if ax is None:\n",
    "        ax = plt.figure(figsize=figsize).add_subplot(111)\n",
    "    ax = fgp.plot(ax=ax,**kwargs)\n",
    "    gp_setAxesScales(ax,option=False)\n",
    "    if not title is None:\n",
    "        gp_setTitle(ax,title)\n",
    "    if mapon:\n",
    "        ctx.add_basemap(ax,  source=ctx.providers.OpenStreetMap.Mapnik)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Read Current IP2DIR Clean Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2983069, 12)\n",
      "    ip_from     ip_to country_code              country_name region_name  \\\n",
      "0  16777216  16777471           US  United States of America  California   \n",
      "1  16777472  16778239           CN                     China      Fujian   \n",
      "2  16778240  16779263           AU                 Australia    Victoria   \n",
      "3  16779264  16781311           CN                     China   Guangdong   \n",
      "4  16781312  16785407           JP                     Japan       Tokyo   \n",
      "\n",
      "   city_name   latitude   longitude  zip_code time_zone ip_from_str  \\\n",
      "0   San Jose  37.339390 -121.894960     95101    -07:00     1.0.0.0   \n",
      "1     Fuzhou  26.061390  119.306110    350004    +08:00     1.0.1.0   \n",
      "2  Melbourne -37.814007  144.963171      3000    +11:00     1.0.4.0   \n",
      "3  Guangzhou  23.127361  113.264570    510140    +08:00     1.0.8.0   \n",
      "4      Tokyo  35.689497  139.692317  160-0021    +09:00    1.0.16.0   \n",
      "\n",
      "    ip_to_str  \n",
      "0   1.0.0.255  \n",
      "1   1.0.3.255  \n",
      "2   1.0.7.255  \n",
      "3  1.0.15.255  \n",
      "4  1.0.31.255  \n",
      "ip_from           int64\n",
      "ip_to             int64\n",
      "country_code     object\n",
      "country_name     object\n",
      "region_name      object\n",
      "city_name        object\n",
      "latitude        float64\n",
      "longitude       float64\n",
      "zip_code         object\n",
      "time_zone        object\n",
      "ip_from_str      object\n",
      "ip_to_str        object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "tdfx = readjoin(IP2DIR,IP2LITE.replace(\".CSV\",\"CLEAN.CSV\"))\n",
    "dumpdf(tdfx)\n",
    "idf = tdfx.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Periodic Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "''' Config '''\n",
    "# cnf = {\n",
    "#     \"52.70.44.155\":\"AWS\",\n",
    "#     \"128.2.208.222\":\"CMU\",\n",
    "#     \"eth1\":\"TMOB\",\n",
    "#     \"enx0016083656d3\":\"CBRS\",\n",
    "#     \"enx0050b623c78d\":\"TMOB\"\n",
    "# }\n",
    "cnf = {\n",
    "    \"54.91.23.217\":\"AWS\",\n",
    "    '52.70.44.155':\"AWS\",\n",
    "    \"18.205.107.100\":\"AWS\",\n",
    "    \"128.2.208.222\":\"CMU\",\n",
    "    \"eth1\":\"TMOB\",\n",
    "    \"enx0016083656d3\":\"CBRS\",\n",
    "    \"enx0050b623c78d\":\"CBRS-5G\"\n",
    "}\n",
    "starttime = datetime.datetime(2024, 4, 18, 9)\n",
    "endtime = datetime.datetime(2024, 4, 23)\n",
    "starttime2 = datetime.datetime(2024, 4, 1, 1)\n",
    "endtime2 = datetime.datetime(2024, 4, 9, 11)\n",
    "plt.rcParams.update({'font.size': 12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iperfiles=2702 pingfiles=7520 trrtfiles=7184\n"
     ]
    }
   ],
   "source": [
    "txtfiles = [os.path.join(DATADIR2,fn) for fn in os.listdir(DATADIR2) if fn.endswith(\".csv\")] +  \\\n",
    "           [os.path.join(ALLMEASDIR,fn) for fn in os.listdir(ALLMEASDIR) if fn.endswith(\".csv\")]\n",
    "# txtfiles = [os.path.join(DATADIR2,fn) for fn in os.listdir(DATADIR2) if fn.endswith(\".csv\")]\n",
    "iperffiles = [fn for fn in txtfiles if \"iperf-\" in fn]\n",
    "pingfiles =  [fn for fn in txtfiles if \"ping-\" in fn ]\n",
    "trrtfiles =  [fn for fn in txtfiles if \"traceroute-\" in fn ]\n",
    "print(f\"iperfiles={len(iperffiles)} pingfiles={len(pingfiles)} trrtfiles={len(trrtfiles)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Periodic Pings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def readPingFile(fn):\n",
    "    fdf = to_ts(readjoin(\"\",fn),format = '%Y-%m-%d %H:%M:%S.%f')\n",
    "    fdf['SDEST'] = fdf.DEST.map(lambda xx: cnf[xx])\n",
    "    fdf['SACCESS'] = fdf.IFC.map(lambda xx: cnf[xx])\n",
    "    # dumpdf(fdf)\n",
    "    return fdf.iloc[1:]\n",
    "\n",
    "# print(pingfiles)\n",
    "tdfx = readPingFile(pingfiles[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tdfy = pd.DataFrame()\n",
    "for fn in pingfiles:\n",
    "    tdfx = readPingFile(fn)\n",
    "    tdfy = pd.concat([tdfy,tdfx])\n",
    "\n",
    "tdfa = tdfy.copy()\n",
    "writejoin(tdfa.set_index(\"TIMESTAMP\"),\".\",\"tmp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdfy = to_ts(readjoin(\".\",\"tmp.csv\"),fmt=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "tdfa = tdfy.copy()\n",
    "dumpdf(tdfy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime = datetime.datetime(2024, 4, 18, 16)\n",
    "endtime = datetime.datetime(2024, 4, 19, 9)\n",
    "starttime2 = datetime.datetime(2024, 3, 24, 0)\n",
    "endtime2 = datetime.datetime(2024, 4, 3, 11)\n",
    "def checkSACCESS(xx):\n",
    "    if xx.TIMESTAMP <= starttime and xx.IFC == \"enx0050b623c78d\":\n",
    "        xx.SACCESS = 'TMOB'\n",
    "    return xx\n",
    "tdfy = tdfa[((tdfa.TIMESTAMP > starttime) & (tdfa.TIMESTAMP < endtime)) |\\\n",
    "            ((tdfa.TIMESTAMP > starttime2) & (tdfa.TIMESTAMP < endtime2))].sort_values('TIMESTAMP').copy()\n",
    "# tdfy = tdfa[((tdfa.TIMESTAMP > starttime) & (tdfa.TIMESTAMP < endtime))].sort_values('TIMESTAMP').copy()\n",
    "tdfy = tdfy.apply(checkSACCESS, axis=1)\n",
    "dumpdf(tdfy[tdfy.IFC ==  \"enx0050b623c78d\"].sort_values('TIMESTAMP'))\n",
    "tdfy = tdfy[((tdfy.SACCESS == \"CBRS-5G\") | (tdfy.SACCESS == \"TMOB\")) & (tdfy.SDEST == \"AWS\")]\n",
    "# title = f\"PING TEST\\n{tdfy.TIMESTAMP.iloc[0].floor('1H')}\\nto {tdfy.TIMESTAMP.iloc[-1].ceil('1H')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def makeTable(fdf,title=\"UNKNOWN\",filename=\"tmp.png\"):\n",
    "    # print(filename)\n",
    "    sdestlst = list(fdf.SDEST.drop_duplicates())\n",
    "    sacclst = list(fdf.SACCESS.drop_duplicates())\n",
    "    print(sdestlst,sacclst)\n",
    "    describe = pd.DataFrame()\n",
    "    for SDEST in sdestlst:\n",
    "        for SACCESS in sacclst:           \n",
    "            fdf1 = fdf[(fdf.SDEST == SDEST) & (fdf.SACCESS == SACCESS)]\n",
    "            fser = np.round(fdf1.TIME.describe(),2)\n",
    "            fser.name = f\"via {SACCESS}\\nto {SDEST}\"\n",
    "            # dumpdf(fser)\n",
    "            describe = pd.concat([describe,fser],axis=1)\n",
    "    describe = describe.reset_index(names=['METRIC'])\n",
    "    fig, ax = plt.subplots()\n",
    "    # hide axes\n",
    "    fig.patch.set_visible(False)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "    table = ax.table(cellText=describe.values, colLabels=describe.columns, loc='center')\n",
    "    cellDict = table.get_celld() # Set header size\n",
    "    tablewidth = len(sdestlst) * len(sacclst) + 1\n",
    "    for ii in range(0,tablewidth):\n",
    "        cellDict[(0,ii)].set_height(.1)\n",
    "    table.scale(1,2)\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(12)\n",
    "    ax.set_title(title,fontsize=14)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    savePlot(ax,filename)\n",
    "title = f\"PING TEST\\n{tdfy.TIMESTAMP.iloc[0].floor('1H')}\\nto {tdfy.TIMESTAMP.iloc[-1].ceil('1H')}\"\n",
    "tdfz = tdfy.copy()[['TIME','TIMESTAMP','SDEST','SACCESS']]\n",
    "makeTable(tdfz,title=title + \"\\nSTATISTICS\",filename=\"PING TEST STATS.png\")\n",
    "\n",
    "colname='ROLLINGTIME'\n",
    "# WINDOW=int(np.round(60/7*4,0))\n",
    "WINDOW=50\n",
    "print(WINDOW,tdfy.TIME.min(),tdfy.TIME.max())\n",
    "def makeLine(fdf,col,title=\"UNKNOWN\", filename=\"tmp.png\"):\n",
    "    sdestlst = list(fdf.SDEST.drop_duplicates())\n",
    "    sacclst = list(fdf.SACCESS.drop_duplicates())\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "    fig, ax = plt.subplots()\n",
    "    legendlabellst = []\n",
    "    for SDEST in sdestlst:\n",
    "        for SACCESS in sacclst:\n",
    "            fdf1 = fdf[(tdfy.SDEST == SDEST) & (tdfy.SACCESS == SACCESS)]\n",
    "            if col == 'ROLLINGTIME':\n",
    "                fdf1[colname] = fdf1['TIME'].rolling(WINDOW).mean()\n",
    "            ax = ts_lineplot(fdf1,[col], ax=ax, title=title,legend=True)\n",
    "            legendlabellst.append(f\"Via {SACCESS} to {SDEST}\")\n",
    "    ax.legend(labels=legendlabellst)\n",
    "    ax.set_title(title,fontsize=12)\n",
    "    ax.set_ylabel(\"Ping Time (ms)\")\n",
    "    savePlot(ax,filename)\n",
    "tdfz = tdfy.copy()\n",
    "makeLine(tdfz,\"TIME\",filename=\"PING LINE TEST.png\",title=title + \"\\nLINE\")\n",
    "makeLine(tdfz,\"ROLLINGTIME\",filename=\"ROLLING PING LINE TEST.png\",title=title + \"\\nROLLING LINE\")\n",
    "\n",
    "def makeHist(fdf,col,title=\"UNKNOWN\", filename=\"tmp.png\",filterdest=None,filteraccess = None):\n",
    "    sdestlst = list(fdf.SDEST.drop_duplicates())\n",
    "    sacclst = list(fdf.SACCESS.drop_duplicates())\n",
    "    # ax = None\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "    fig, ax = plt.subplots()\n",
    "    legendlabellst = []\n",
    "    for SDEST in sdestlst:\n",
    "        if filterdest is not None and filterdest != SDEST: continue\n",
    "        for SACCESS in sacclst:\n",
    "            if filteraccess is not None and filteraccess != SACCESS: continue\n",
    "            fdf1 = fdf[(tdfy.SDEST == SDEST) & (tdfy.SACCESS == SACCESS)]\n",
    "            ax = histplot(fdf1.TIME, ax=ax, title=title, tabon=False, legend=True)\n",
    "            legendlabellst.append(f\"Via {SACCESS} to {SDEST}\")\n",
    "    ax.legend(labels = legendlabellst,fontsize=12)\n",
    "    ax.set_title(title,fontsize = 12)\n",
    "    ax.set_xlabel(\"Ping Time (ms)\")\n",
    "    savePlot(ax,filename)\n",
    "    \n",
    "tdfz = tdfy.copy()\n",
    "makeHist(tdfz,\"TIME\",filename=\"PING HIST TEST.png\",title=title + \"\\nHISTOGRAM\")\n",
    "# makeHist(tdfz,\"TIME\",filename=\"PING HIST TEST.png\",title=title + \"\\nHISTOGRAM\",filterdest = \"AWS\",filteraccess=\"TMOB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = f\"PING TEST\\n{tdfy.TIMESTAMP.iloc[0].floor('1H')}\\nto {tdfy.TIMESTAMP.iloc[-1].ceil('1H')}\"\n",
    "tdfz = tdfy.copy()[['TIME','TIMESTAMP','SDEST','SACCESS']]\n",
    "\n",
    "\n",
    "tdfz = tdfz[(tdfz.SACCESS == \"CBRS-5G\") | (tdfz.SACCESS == \"TMOB\")]\n",
    "makeTable(tdfz,title=title + \"\\nSTATISTICS\",filename=\"PING TEST STATS.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Periodic Iperf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 9)\n",
      "   THROUGHPUT                  TIMESTAMP                       HDATE   TEST  \\\n",
      "0        8.50 2024-04-18 10:33:43.396126  2024-04-18-10-33-43-396126  iperf   \n",
      "1        9.75 2024-04-18 10:33:43.396126  2024-04-18-10-33-43-396126  iperf   \n",
      "2       11.40 2024-04-18 10:33:43.396126  2024-04-18-10-33-43-396126  iperf   \n",
      "3       12.40 2024-04-18 10:33:43.396126  2024-04-18-10-33-43-396126  iperf   \n",
      "4       12.60 2024-04-18 10:33:43.396126  2024-04-18-10-33-43-396126  iperf   \n",
      "\n",
      "           DEST              IFC DIRECTION SDEST  SACCESS  \n",
      "0  54.91.23.217  enx0050b623c78d      DOWN   AWS  CBRS-5G  \n",
      "1  54.91.23.217  enx0050b623c78d      DOWN   AWS  CBRS-5G  \n",
      "2  54.91.23.217  enx0050b623c78d      DOWN   AWS  CBRS-5G  \n",
      "3  54.91.23.217  enx0050b623c78d      DOWN   AWS  CBRS-5G  \n",
      "4  54.91.23.217  enx0050b623c78d      DOWN   AWS  CBRS-5G  \n",
      "THROUGHPUT           float64\n",
      "TIMESTAMP     datetime64[ns]\n",
      "HDATE                 object\n",
      "TEST                  object\n",
      "DEST                  object\n",
      "IFC                   object\n",
      "DIRECTION             object\n",
      "SDEST                 object\n",
      "SACCESS               object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "def readIPerfFile(fn):\n",
    "    fdf = to_ts(readjoin(\"\",fn),format = '%Y-%m-%d %H:%M:%S.%f')\n",
    "    fdf['SDEST'] = fdf.DEST.map(lambda xx: cnf[xx])\n",
    "    fdf['SACCESS'] = fdf.IFC.map(lambda xx: cnf[xx])\n",
    "    # dumpdf(fdf)\n",
    "    return fdf\n",
    "\n",
    "# print(pingfiles)\n",
    "tdfx = readIPerfFile(iperffiles[1])\n",
    "dumpdf(tdfx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m tdfy \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m iperffiles:\n\u001b[1;32m----> 3\u001b[0m     tdfx \u001b[38;5;241m=\u001b[39m \u001b[43mreadIPerfFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     tdfy \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([tdfy,tdfx])\n",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m, in \u001b[0;36mreadIPerfFile\u001b[1;34m(fn)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadIPerfFile\u001b[39m(fn):\n\u001b[1;32m----> 2\u001b[0m     fdf \u001b[38;5;241m=\u001b[39m to_ts(\u001b[43mreadjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m,\u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS.\u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m     fdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSDEST\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m fdf\u001b[38;5;241m.\u001b[39mDEST\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m xx: cnf[xx])\n\u001b[0;32m      4\u001b[0m     fdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSACCESS\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m fdf\u001b[38;5;241m.\u001b[39mIFC\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m xx: cnf[xx])\n",
      "File \u001b[1;32m~\\Git\\PyUtils\\lib\\pdutils.py:135\u001b[0m, in \u001b[0;36mreadjoin\u001b[1;34m(dn, fn, **kwargs)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(ffn):\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m ffn)\n\u001b[1;32m--> 135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mffn\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\py38\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\py38\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\.conda\\envs\\py38\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\py38\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1679\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1676\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1678\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1679\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1680\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1681\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\py38\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\py38\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:550\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\.conda\\envs\\py38\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:639\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\.conda\\envs\\py38\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:850\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\.conda\\envs\\py38\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\.conda\\envs\\py38\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:2029\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "tdfy = pd.DataFrame()\n",
    "for fn in iperffiles:\n",
    "    tdfx = readIPerfFile(fn)\n",
    "    tdfy = pd.concat([tdfy,tdfx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tdfy = tdfy[tdfy.TIMESTAMP > starttime].sort_values('TIMESTAMP')\n",
    "\n",
    "title = f\"IPERF TEST\\n{tdfy.TIMESTAMP.iloc[0].floor('1H')}\\nto {tdfy.TIMESTAMP.iloc[-1].ceil('1H')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "colname='ROLLING THROUGHPUT'\n",
    "DIRECTION='UP'\n",
    "# dumpdf(tdfy)\n",
    "WINDOW=int(np.round(60/7*4,0))\n",
    "WINDOW=100\n",
    "print(WINDOW,tdfy.THROUGHPUT.min(),tdfy.THROUGHPUT.max())\n",
    "tdfz = tdfy.copy()[['THROUGHPUT','TIMESTAMP','SDEST','SACCESS','DIRECTION','HDATE']]\n",
    "if colname != \"THROUGHPUT\":\n",
    "    tdfz[colname] = tdfz['THROUGHPUT'].rolling(WINDOW).mean()\n",
    "tdfz['DIRECTION'] = tdfz.DIRECTION.map(lambda xx: \"UP\" if xx == np.nan else xx)\n",
    "tdfz = tdfz[tdfz.DIRECTION == DIRECTION]\n",
    "\n",
    "# dumpdf(tdfy)\n",
    "def makeLine(fdf,col,title=\"UNKNOWN\", filename=\"tmp.png\"):\n",
    "    sdestlst = list(fdf.SDEST.drop_duplicates())\n",
    "    sacclst = list(fdf.SACCESS.drop_duplicates())\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "    fig, ax = plt.subplots()\n",
    "    legendlabellst = []\n",
    "    for SDEST in sdestlst:\n",
    "        for SACCESS in sacclst:\n",
    "            fdf1 = fdf[(fdf.SDEST == SDEST) & (fdf.SACCESS == SACCESS)]\n",
    "            ax = ts_lineplot(fdf1,[col], ax=ax, title=title,legend=True)\n",
    "            legendlabellst.append(f\"{SDEST} {SACCESS}\")\n",
    "    ax.legend(labels=legendlabellst,fontsize=12)\n",
    "\n",
    "    savePlot(ax,filename)\n",
    "title = f\"IPERF TEST {colname} {DIRECTION} {tdfz.HDATE.min()}\\nto {tdfz.HDATE.max()} \"\n",
    "makeLine(tdfz,colname,filename=f\"IPERF {DIRECTION} TEST LINE.png\",title=title + \"\\nLINE\")\n",
    "\n",
    "def makeHist(fdf,col,title=\"UNKNOWN\", filename=\"tmp.png\"):\n",
    "    sdestlst = list(fdf.SDEST.drop_duplicates())\n",
    "    sacclst = list(fdf.SACCESS.drop_duplicates())\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "    fig, ax = plt.subplots()\n",
    "    legendlabellst = []\n",
    "    for SDEST in sdestlst:\n",
    "        for SACCESS in sacclst:\n",
    "            fdf1 = fdf[(fdf.SDEST == SDEST) & (fdf.SACCESS == SACCESS)]\n",
    "            ax = histplot(fdf1[col], ax=ax, title=title, tabon=False, legend=True)\n",
    "            legendlabellst.append(f\"{SDEST} {SACCESS}\")\n",
    "    ax.legend(labels = legendlabellst,fontsize=12)\n",
    "    ax.set_title(title,fontsize=12)\n",
    "    savePlot(ax,filename)\n",
    "\n",
    "title = f\"IPERF TEST THROUGHPUT {DIRECTION} {tdfz.HDATE.min()}\\nto {tdfz.HDATE.max()} \"\n",
    "makeHist(tdfz,\"THROUGHPUT\",filename=f\"IPERF {DIRECTION} TEST HIST.png\",title=title + \"\\nHISTOGRAM\")\n",
    "\n",
    "def makeTable(fdf,col,title=\"UNKNOWN\",filename=\"tmp.png\"):\n",
    "    print(filename)\n",
    "    sdestlst = list(fdf.SDEST.drop_duplicates())\n",
    "    sacclst = list(fdf.SACCESS.drop_duplicates())\n",
    "    describe = pd.DataFrame()\n",
    "    for SDEST in sdestlst:\n",
    "        for SACCESS in sacclst:\n",
    "            fdf1 = fdf[(fdf.SDEST == SDEST) & (fdf.SACCESS == SACCESS)]\n",
    "            fser = np.round(fdf1.THROUGHPUT.describe(),2)\n",
    "            fser.name = f\"{SDEST} {SACCESS}\"\n",
    "            dumpdf(fser)\n",
    "            describe = pd.concat([describe,fser],axis=1)\n",
    "    describe = describe.reset_index(names=['METRIC'])\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "    fig, ax = plt.subplots()\n",
    "    # hide axes\n",
    "    fig.patch.set_visible(False)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "    table = ax.table(cellText=describe.values, colLabels=describe.columns, loc='center')\n",
    "    table.scale(1,2)\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(12)\n",
    "    ax.set_title(title + \" STATISTICS\",fontsize=12)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    savePlot(ax,filename)\n",
    "    \n",
    "title = f\"IPERF TEST THROUGHPUT {DIRECTION} {tdfz.HDATE.min()}\\nto {tdfz.HDATE.max()} \"\n",
    "makeTable(tdfz,\"THROUGHPUT\",title=title + \"\\nSTATISTICS\" ,filename=f\"IPERF {DIRECTION} TEST STATS.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Periodic Traceroute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def readTraceRouteFile(fn):\n",
    "    fdf = to_ts(readjoin(\"\",fn),format = '%Y-%m-%d %H:%M:%S.%f')\n",
    "    fdf['FULLCOUNT'] = fdf.shape[0]\n",
    "    # dumpdf(fdf)\n",
    "    return fdf\n",
    "\n",
    "tdfx = readTraceRouteFile(trrtfiles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m tdfy \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m trrtfiles:\n\u001b[1;32m----> 3\u001b[0m     tdfx \u001b[38;5;241m=\u001b[39m \u001b[43mreadTraceRouteFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     tdfy \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([tdfy,tdfx])\n\u001b[0;32m      5\u001b[0m tdfy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSDEST\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m tdfy\u001b[38;5;241m.\u001b[39mDEST\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m xx: cnf[xx])\n",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m, in \u001b[0;36mreadTraceRouteFile\u001b[1;34m(fn)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadTraceRouteFile\u001b[39m(fn):\n\u001b[1;32m----> 2\u001b[0m     fdf \u001b[38;5;241m=\u001b[39m to_ts(\u001b[43mreadjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m,\u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS.\u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m     fdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFULLCOUNT\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m fdf\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# dumpdf(fdf)\u001b[39;00m\n",
      "File \u001b[1;32m~\\Git\\PyUtils\\lib\\pdutils.py:135\u001b[0m, in \u001b[0;36mreadjoin\u001b[1;34m(dn, fn, **kwargs)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(ffn):\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m ffn)\n\u001b[1;32m--> 135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mffn\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\py38\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\py38\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\.conda\\envs\\py38\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\py38\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1679\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1676\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1678\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1679\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1680\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1681\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\py38\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\py38\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:550\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\.conda\\envs\\py38\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:639\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\.conda\\envs\\py38\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:850\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\.conda\\envs\\py38\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\.conda\\envs\\py38\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:2029\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "tdfy = pd.DataFrame()\n",
    "for fn in trrtfiles:\n",
    "    tdfx = readTraceRouteFile(fn)\n",
    "    tdfy = pd.concat([tdfy,tdfx])\n",
    "tdfy['SDEST'] = tdfy.DEST.map(lambda xx: cnf[xx])\n",
    "tdfy['SACCESS'] = tdfy.IFC.map(lambda xx: cnf[xx])\n",
    "tdfy['NIP'] = tdfy.IP.map(ipadd2ipno)\n",
    "dumpdf(tdfy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tdfy = tdfy[tdfy.TIMESTAMP > starttime].sort_values('TIMESTAMP')\n",
    "title = f\"TRACEROUTE TEST\\n{tdfy.TIMESTAMP.iloc[0].floor('1H')}\\nto {tdfy.TIMESTAMP.iloc[-1].ceil('1H')}\"\n",
    "print(title)\n",
    "dumpdf(tdfy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "retcols = ['country_code','region_name','city_name','latitude','longitude']\n",
    "nullser =  pd.Series([None for col in retcols],index=retcols)\n",
    "nulllst = nullser.to_list()\n",
    "# print(nullser)\n",
    "def fetchIPdata(nip):\n",
    "    tdf = idf[(idf.ip_from <= nip ) & (idf.ip_to >= nip)  & (idf.latitude != 0 ) ]\n",
    "    if tdf.shape[0] == 0: return nulllst\n",
    "    retlst = tdf[retcols].iloc[0].to_list()\n",
    "    return retlst\n",
    "\n",
    "tdfx = tdfy.copy()[:]\n",
    "tdfx[retcols] = pd.DataFrame(tdfx.NIP.map(fetchIPdata).tolist(), index= tdfx.index)\n",
    "tdfx = tdfx.dropna()\n",
    "''' Mark each row with the number of GEO IP addresses in that traceroute '''\n",
    "tdfx['GEOCOUNT'] = tdfx.groupby('HDATE')['HDATE'].transform('count')\n",
    "dumpdf(tdfx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "writejoin(tdfx.set_index('TIMESTAMP'),DATADIR2,\"TRACEROUTE_SAVE.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read back from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tdfx = to_ts(readjoin(DATADIR2,\"TRACEROUTE_SAVE.csv\"))\n",
    "dumpdf(tdfx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def runIPdf(fdf):\n",
    "    fdf['geometry'] = pt2geom(fdf,latcol='latitude',lngcol='longitude')\n",
    "    fdf['geometrynext'] = fdf['geometry'].shift(-1)\n",
    "    fdf['region_name_next'] = fdf['region_name'].shift(-1)\n",
    "    fdf['city_name_next'] = fdf['city_name'].shift(-1)\n",
    "    fdf = fdf.dropna()\n",
    "    # print(fdf.shape[0],fdf)\n",
    "    tgp = df2gp(fdf.copy())\n",
    "    tgp['LINEGEO'] = tgp.apply(lambda row: LineString([row['geometry'],row['geometrynext']]), axis = 1)\n",
    "    # print(type(tgp))\n",
    "    return tgp\n",
    "tgpa = runIPdf(tdfx).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def plotMap(fgp,title=\"NONE\",saveon=False,filename=\"tmp.png\"):\n",
    "    tablecolumns = ['city_name','region_name','IP']\n",
    "    tabdf = fgp[tablecolumns]\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "    # plt.figure(figsize=(20,20))\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.subplots_adjust(left=0.2, right=0.8, bottom=0.3, top=0.9)\n",
    "    # fig.subplots_adjust(hspace=30.0)\n",
    "    ax= gp_plotPoints(usagb,ax=ax, mapon=True, alpha=0)\n",
    "    ax = gp_plotPoints(fgp,ax=ax, mapon=True,c='red',title=title)\n",
    "    fgp = fgp[:-1] # Remove the last line -- belongs to next group\n",
    "    ax = gp_plotLines(fgp,mapon=True,geocol='LINEGEO',ax=ax,color='green')\n",
    "    table = ax.table(cellText=tabdf.values, colLabels=tabdf.columns,cellLoc='center', loc='bottom')\n",
    "    table.scale(1,1)\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    if saveon:\n",
    "        savePlot(ax,filename)\n",
    "\n",
    "''' Slice Plot '''\n",
    "tgpb = tgpa.copy()\n",
    "tgroup = tgpb.groupby('HDATE')\n",
    "print(f\"NUMBER OF TRACEROUTES: {tgroup.ngroups}\")\n",
    "pltstart = 0\n",
    "pltlimit = pltstart + 3\n",
    "for ii, (name, group) in enumerate(tgroup):\n",
    "    if ii >= pltstart:\n",
    "        print(ii,name)\n",
    "        # dumpdf(group,head=group.shape[0])\n",
    "        title = f\"{group.SACCESS.iloc[0]} to {group.SDEST.iloc[0]} run={ii} allhop={group.FULLCOUNT.iloc[0]} geohops={group.GEOCOUNT.iloc[0]}\\n{group.HDATE.iloc[0]}\"\n",
    "        filename=f\"TRACEROUTE_MAP_{group.SACCESS.iloc[0]}_{group.SDEST.iloc[0]}_RUN_{ii}_{group.HDATE.iloc[0]}.png\"\n",
    "        plotMap(group[1:],title=title,filename=filename,saveon=True)\n",
    "    if ii >= pltlimit: break\n",
    "# dumpdf(tgpb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "''' Filter Plot '''\n",
    "saccess = \"CBRS\"\n",
    "sdest = \"AWS\"\n",
    "tgpb = tgpa.copy()[(tgpa.SACCESS == saccess) & (tgpa.SDEST == sdest)]\n",
    "tgroup = tgpb.groupby('HDATE')\n",
    "print(f\"NUMBER OF TRACEROUTES: {tgroup.ngroups}\")\n",
    "pltstart = 24\n",
    "pltlimit = pltstart + 4\n",
    "for ii, (name, group) in enumerate(tgroup):\n",
    "    if ii >= pltstart:\n",
    "        print(ii,name)\n",
    "        title = f\"{group.SACCESS.iloc[0]} to {group.SDEST.iloc[0]} run={ii} allhop={group.FULLCOUNT.iloc[0]} geohops={group.GEOCOUNT.iloc[0]}\\n{group.HDATE.iloc[0]}\"\n",
    "        filename=f\"TRACEROUTE_MAP_{group.SACCESS.iloc[0]}_{group.SDEST.iloc[0]}_RUN_{ii}_{group.HDATE.iloc[0]}.png\"\n",
    "        plotMap(group[1:],title=title,filename=filename,saveon=True)\n",
    "    if ii >= pltlimit: break\n",
    "# dumpdf(tgpb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def makeHist(fdf,col,title=\"UNKNOWN\", filename=\"tmp.png\"):\n",
    "    sdestlst = list(fdf.SDEST.drop_duplicates())\n",
    "    sacclst = list(fdf.SACCESS.drop_duplicates())\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "    fig, ax = plt.subplots()\n",
    "    legendlabellst = []\n",
    "    for SDEST in sdestlst:\n",
    "        for SACCESS in sacclst:\n",
    "            print(f\"{SDEST} {SACCESS}\")\n",
    "            fdf1 = fdf[(fdf.SDEST == SDEST) & (fdf.SACCESS == SACCESS)]\n",
    "            ax = histplot(fdf1[col], ax=ax, title=title, tabon=False, legend=True,fontsize=12)\n",
    "            legendlabellst.append(f\"{SDEST} {SACCESS}\")\n",
    "    ax.legend(labels = legendlabellst,fontsize=12)\n",
    "    savePlot(ax,filename)\n",
    "filename=f\"TRACEROUTE_HISTOGRAM_{tdfx.HDATE.iloc[0]}_{tdfx.HDATE.iloc[-1]}.png\"\n",
    "dumpdf(tdfx[['SDEST','SACCESS','FULLCOUNT','HDATE']])\n",
    "ax = makeHist(tdfx[['SDEST','SACCESS','FULLCOUNT','HDATE']].drop_duplicates('HDATE'),'FULLCOUNT', title=f\"TRACERT FULLCOUNT RESULTS\", filename=\"FULLCOUNT_\"+filename)\n",
    "# ax = makeHist(tdfx[['SDEST','SACCESS','GEOCOUNT']],'GEOCOUNT', title=f\"TRACERT GEOCOUNT RESULTS\",filename=\"GEOCOUNT_\"+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "colname = 'FULLCOUNT'\n",
    "tdfy = tdfx.copy()\n",
    "tdfy['PRIVATECOUNT'] = tdfy.FULLCOUNT - tdfy.GEOCOUNT\n",
    "tdfy = tdfy.drop_duplicates(['HDATE',colname])\n",
    "\n",
    "WINDOW=60\n",
    "print(WINDOW,tdfy[colname].min(),tdfy[colname].max())\n",
    "# dumpdf(tdfy)\n",
    "def makeLine(fdf,col,title=\"UNKNOWN\", filename=\"tmp.png\"):\n",
    "    sdestlst = list(fdf.SDEST.drop_duplicates())\n",
    "    sacclst = list(fdf.SACCESS.drop_duplicates())\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "    fig, ax = plt.subplots()\n",
    "    legendlabellst = []\n",
    "    for SDEST in sdestlst:\n",
    "        for SACCESS in sacclst:\n",
    "            fdf1 = fdf[(tdfy.SDEST == SDEST) & (tdfy.SACCESS == SACCESS)]\n",
    "            ax = ts_lineplot(fdf1,[col], ax=ax, title=f\"TRACEROUTE RESULTS {col}\",legend=True)\n",
    "            legendlabellst.append(f\"{SDEST} {SACCESS}\")\n",
    "    ax.legend(labels=legendlabellst,fontsize=12)\n",
    "\n",
    "    # savePlot(ax,filename)\n",
    "\n",
    "makeLine(tdfy,colname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeTable(fdf,title=\"UNKNOWN\",filename=\"tmp.png\"):\n",
    "    print(filename)\n",
    "    sdestlst = list(fdf.SDEST.drop_duplicates())\n",
    "    sacclst = list(fdf.SACCESS.drop_duplicates())\n",
    "    describe = pd.DataFrame()\n",
    "    for SDEST in sdestlst:\n",
    "        for SACCESS in sacclst:\n",
    "            fdf1 = fdf[(tdfy.SDEST == SDEST) & (tdfy.SACCESS == SACCESS)]\n",
    "            fser = np.round(fdf1.FULLCOUNT.describe(),2)\n",
    "            fser.name = f\"{SDEST} {SACCESS}\"\n",
    "            dumpdf(fser)\n",
    "            describe = pd.concat([describe,fser],axis=1)\n",
    "    describe = describe.reset_index(names=['METRIC'])\n",
    "    plt.rcParams.update({'font.size': 12})\n",
    "    fig, ax = plt.subplots()\n",
    "    # hide axes\n",
    "    fig.patch.set_visible(False)\n",
    "    ax.axis('off')\n",
    "    ax.axis('tight')\n",
    "    table = ax.table(cellText=describe.values, colLabels=describe.columns, loc='center')\n",
    "    table.scale(1,2)\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(12)\n",
    "    ax.set_title(title,fontsize=12)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    savePlot(ax,filename)\n",
    "title = f\"TRACEROUTE TEST\\n{tdfy.TIMESTAMP.iloc[0].floor('1H')}\\nto {tdfy.TIMESTAMP.iloc[-1].ceil('1H')}\"\n",
    "makeTable(tdfy,title=title + \" STATISTICS\" ,filename=\"TRACEROUTE TEST STATS.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dumpdf(tdfy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
